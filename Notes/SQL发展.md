# mysql/postgresql(SQL) ==> redis/mongodb/hbase(NoSQL) ==> Google Spanner/F1/TiDB(newSQL)

### MySQL
第一个介绍的单机 RDBMS 就是 MySQL。相信大多数朋友都已经对 MySQL 非常熟悉，基本上 MySQL 的成长史就是互联网的成长史。我接触的第一个 MySQL 版本是 MySQL 4.0，到后来的 MySQL 5.5 更是经典——基本所有的互联网公司都在使用。MySQL 也普及了「可插拔」引擎这一概念，针对不同的业务场景选用不同的存储引擎是 MySQL tuning 的一个重要的方式。比如对于有事务需求的场景使用 InnoDB；对于并发读取的场景 MyISAM 可能比较合适；但是现在我推荐绝大多数情况还是使用 InnoDB，毕竟 5.6 后已经成为了官方的默认引擎。大多数朋友都基本知道什么场景适用 MySQL（几乎所有需要持久化结构化数据的场景），我就不赘述了。

另外值得一提的是 MySQL 5.6 中引入了多线程复制和 GTID，使得故障恢复和主从的运维变得比较方便。另外，5.7（目前处于 GA 版本） 是 MySQL 的一个重大更新，主要是读写性能和复制性能上有了长足的进步（在5.6版本中实现了SCHEMA级别的并行复制，不过意义不大，倒是MariaDB的多线程并行复制大放异彩，有不少人因为这个特性选择MariaDB。MySQL 5.7 MTS支持两种模式，一种是和5.6一样，另一种则是基于binlog group commit实现的多线程复制，也就是MASTER上同时提交的binlog在SLAVE端也可以同时被apply，实现并行复制）。如果有单机数据库技术选型的朋友，基本上只需要考虑 5.7 或者 MariaDB 就好了，而且 5.6、5.7 由 Oracle 接手后，性能和稳定性上都有了明显的提升。

### PostgreSQL
PostgreSQL 的历史也非常悠久，其前身是 UCB 的 Ingres，主持这个项目的 Michael Stronebraker 于 2015 年获得图灵奖。后来项目更名为 Post-Ingres，项目基于 BSD license 下开源。 1995 年几个 UCB 的学生为 Post-Ingres 开发了 SQL 的接口，正式发布了 PostgreSQL95，随后一步步在开源社区中成长起来。和 MySQL 一样，PostgreSQL 也是一个单机的关系型数据库，但是与 MySQL 方便用户过度扩展的 SQL 文法不一样的是，PostgreSQL 的 SQL 支持非常强大，不管是内置类型、JSON 支持、GIS 类型以及对于复杂查询的支持，PL/SQL 等都比 MySQL 强大得多，而且从代码质量上来看，PostgreSQL 的代码质量是优于 MySQL 的，另外相对于MySQL 5.7以前的版本，PostgreSQL 的 SQL 优化器比 MySQL 强大很多，几乎所有稍微复杂的查询 PostgreSQL 的表现都优于 MySQL。

从近几年的趋势上来看，PostgreSQL 的势头也很强劲，我认为 PostgreSQL 的不足之处在于没有 MySQL 那样强大的社区和群众基础。MySQL 经过那么多年的发展，积累了很多的运维工具和最佳实践，但是 PostgreSQL 作为后起之秀，拥有更优秀的设计和更丰富的功能。PostgreSQL 9 以后的版本也足够稳定，在做新项目技术选型的时候，是一个很好的选择。另外也有很多新的数据库项目是基于 PostgreSQL 源码的基础上进行二次开发，比如 Greenplum 等。

我认为，单机数据库的时代很快就会过去。摩尔定律带来的硬件红利总是有上限的，现代业务的数据规模、流量以及现代的数据科学对于数据库的要求，单机已经很难满足。比如，网卡磁盘 IO 和 CPU 总有瓶颈，线上敏感的业务系统可能还得承担 SPOF（单点故障） 的风险，主从复制模型在主挂掉时到底切还是不切？切了以后数据如何恢复？如果只是出现主从机器网络分区问题呢？甚至是监控环境出现网络分区问题呢？这些都是单机数据库面临的巨大挑战。所以我的观点是，无论单机性能多棒（很多令人乍舌的评测数据都是针对特定场景的优化，另外甚至有些都是本机不走网络，而大多数情况数据库出现的第一个瓶颈其实是网卡和并发连接……），随着互联网的蓬勃发展和移动互联网的出现，数据库系统迎来了第一次分布式的洗礼。

### 分布式时代：NoSQL 的复兴和模型简化的力量
在介绍 NoSQL 之前，我想提两个公司，一个是 Google，另一个是 Amazon。

### Google
Google 应该是第一个将分布式存储技术应用到大规模生产环境的公司，同时也是在分布式系统上积累最深的公司，可以说目前工业界的分布式系统的工程实践及思想大都来源于 Google。比如 2003 年的 GFS 开创了分布式文件系统，2006 年的 Bigtable 论文开创了分布式键值系统，直接催生的就是 Hadoop 的生态；至于 2012 年发表论文的 Spanner 和 F1更是一个指明未来关系型数据库发展方向的里程碑式的项目，这个我们后续会说。

### Amazon
另一个公司是 Amazon。2007 年发表的 Dynamo的论文 尝试引入了最终一致性的概念， WRN 的模型及向量时钟的应用，同时将一致性 HASH、merkle tree 等当时一些很新潮的技术整合起来，正式标志着 NoSQL 的诞生。NoSQL 对后来业界的影响非常大，后来的 Cassandra、RiakDB、Voldemort 等数据库都是基于 Dynamo 的设计发展起来的。

### NoSQL
说完了几个比较著名的存储引擎，我们来讲讲比较著名的 NoSQL。在我的定义中，NoSQL 是Not Only SQL 的缩写，所以可能包含的范围有内存数据库，持久化数据库等，总之就是和单机的关系型数据库不一样的结构化数据存储系统。

我们先从缓存开始。

### memcached
前面提到了 memcached 应该是第一个大规模在业界使用的缓存数据库，memcached 的实现极其简单，相当于将内存用作大的 HASH Table，只能在上面进行 get/set/ 计数器等操作，在此之上用 libevent 封装了一层网络层和文本协议（也有简单的二进制协议），虽然支持一些 CAS 的操作，但是总体上来看，还是非常简单的。但是 memcached 的内存利用率并不太高，这是因为 memcached 为了避免频繁申请内存导致的内存碎片的问题，采用了自己实现的 slab allocator 的方式。即内存的分配都是一块一块的，最终存储在固定长度的 chunk 上，内存最小的分配单元是 chunk，另外 libevent 的性能也并没有优化到极致。但是这些缺点并不妨碍 memcached 成为当时的开源缓存事实标准。另外，八卦一下，memcached 的作者 Brad Fitzpatrick 现在在 Google，大家如果用 Golang 的话，Go 的官方 HTTP 包就是这哥们写的，是个很高产的工程师。

### Redis
如果我没记错的话，在 2009 年前后，一位意大利的工程师 Antirez ，开源了 Redis。从此彻底颠覆了缓存的市场，到现在大多数缓存的业务都已用上 Redis，memcached 基本退出了历史舞台。Redis 最大的特点是拥有丰富的数据结构支持，不仅仅是简单的 Key-Value，还包括队列、集合、Sorted Set 等等，提供了非常丰富的表达力，而且 Redis 还提供 sub/pub 等超出数据库范畴的便捷功能，使得几乎一夜之间大家纷纷投入 Redis 的怀抱。

### MongoDB
在 NoSQL 的大家庭中，MongoDB 其实是一个异类，大多 NoSQL 舍弃掉 SQL 是为了追求更极致的性能和可扩展能力，而 MongoDB 主动选择了文档作为对外的接口，非常像 JSON 的格式。Schema-less 的特性对于很多轻量级业务和快速变更的互联网业务意义很大，而且 MongoDB 的易用性很好，基本做到了开箱即用，开发者不需要费心研究数据的表结构，只需要往里存就好了，这确实笼络了一大批开发者。

尽管 MongoDB 早期的版本各种不稳定，性能也不太好（早期的 Mongo 并没有存储引擎，直接使用了 mmap 文件），集群模式还全是问题（比如至今还未解决的 Cluster 同步带宽占用过多的问题），但是因为确实太方便了，在早期的项目快速迭代中，Mongo 是一个不错的选择。但是这也正是它的问题，我不止一次听到当项目变得庞大或者「严肃」的时候，团队最后还是回归了关系型数据库。Anyway，在 2014 年底 MongoDB 收购了 WiredTiger 后，在 2.8 版本中正式亮相，同时 3.0 版本后更是作为默认存储引擎提供，性能和稳定性有了非常大的提升。

但是，从另一方面讲，Schema-less 到底对软件工程是好事还是坏事这个问题还是有待商榷。我个人是站在 Schema 这边的，不过在一些小项目或者需要快速开发的项目中使用 Mongo 确实能提升很多的开发效率，这是毋庸置疑的。

### HBase
说到 NoSQL 不得不提的是 HBase，Hbase 作为 Hadoop 旗下的重要产品，Google Bigtable 的正统开源实现，是不是有一种钦定的感觉 :)。Bigtable 是 Google 内部广泛使用的分布式数据库，接口也不是简单的Key-Value，按照论文的说法叫：multi-dimensional sorted map，也就是 Value 是按照列划分的。Bigtable 构建在 GFS 之上，弥补了分布式文件系统对于海量、小的、结构化数据的插入、更新以及随机读请求的缺陷。

HBase 就是这么一个系统的实现，底层依赖 HDFS。HBase 本身并不实际存储数据，持久化的日志和 SST file (HBase 也是 LSM-Tree 的结构) 直接存储在 HDFS 上，Region Server (RS) 维护了 MemTable 以提供快速的查询，写入都是写日志，后台进行 Compact，避免了直接随机读写 HDFS。数据通过 Region 在逻辑上进行分割，负载均衡通过调节各个 Region Server 负责的 Region 区间实现。当某 Region 太大时，这个 Region 会分裂，后续可能由不同的 RS 负责，但是前面提到了，HBase 本身并不存储数据，这里的 Region 仅是逻辑上的，数据还是以文件的形式存储在 HDFS 上，所以 HBase 并不关心 Replication 、水平扩展和数据的分布，统统交给 HDFS 解决。

和 Bigtable 一样，HBase 提供行级的一致性，严格来说在 CAP 理论中它是一个 CP 的系统，但遗憾的是并没有更进一步提供 ACID 的跨行事务。HBase 的好处就不用说了，显而易见，通过扩展 RS 可以几乎线性提升系统的吞吐，及 HDFS 本身就具有的水平扩展能力。

但是缺点仍然是有的。首先，Hadoop 的软件栈是 Java，JVM 的 GC Tuning 是一个非常烦人的事情，即使已经调得很好了，平均延迟也得几十毫秒；另外在架构设计上，HBase 本身并不存储数据，所以可能造成客户端请求的 RS 并不知道数据到底存在哪台 HDFS DataNode 上，凭空多了一次 RPC；第三，HBase 和 Bigtable 一样，并不支持跨行事务，在 Google 内部不停的有团队基于 Bigtable 来做分布式事务的支持，比如 MegaStore、Percolator。后来 Jeff Dean 有次接受采访也提到非常后悔没有在 Bigtable 中加入跨行事务，不过还好这个遗憾在 Spanner 中得到了弥补，这个一会儿说。总体来说，HBase 还是一个非常健壮且久经考验的系统，但是需要你有对于 Java 和 Hadoop 比较深入的了解后，才能玩转，这也是 Hadoop 生态的一个问题，易用性真是不是太好，而且社区演进速度相对缓慢，也是因为历史包袱过重的缘故吧。

### 中间件与分库分表
NoSQL 就先介绍到这里，接下来我想说的是一些在基于单机关系型数据库之上的中间件和分库分表方案。

这些技术确实历史悠久，而且也是没有办法的选择。关系型数据库不比 Redis，并不是简单的写一个类似 Twemproxy 的中间件就搞定了。数据库的中间件需要考虑很多，比如解析 SQL，解析出 sharding key，然后根据 sharding key 分发请求，再合并；另外数据库有事务，在中间件这层还需要维护 Session 及事务状态，而且大多数方案并没有办法支持跨 shard 的事务。这就不可避免的导致了业务使用起来会比较麻烦，需要重写代码，而且会增加逻辑的复杂度，更别提动态的扩容缩容和自动的故障恢复了。在集群规模越来越大的情况下，运维和 DDL 的复杂度是指数级上升的。

#### 中间件项目盘点
数据库中间件最早的项目大概是 MySQL Proxy，用于实现读写分离。后来国人在这个领域有过很多著名项目，比如阿里的 Cobar 和 TDDL（并未完全开源）；后来社区基于 Cobar 改进的 MyCAT、360 开源的 Atlas 等，都属于这一类中间件产品；在中间件这个方案上基本走到头的开源项目应该是 Youtube 的 Vitess。Vitess 基本上是一个集大成的中间件产品，内置了热数据缓存、水平动态分片、读写分离等等，但是代价也是整个项目非常复杂，另外文档也不太好。大概1年多以前，我们尝试搭建起完整的 Vitess 集群，但是并未成功，可见其复杂度。

另外一个值得一提的是 Postgres-XC 这个项目，Postgres-XC 的野心还是很大的，整体的架构有点像早期版本的 OceanBase，由一个中央节点来处理协调分布式事务 / 解决冲突，数据分散在各个存储节点上，应该是目前 PostgreSQL 社区最好的分布式扩展方案。其他的就不提了。

### 未来在哪里？NewSQL！
一句话，NewSQL 就是未来。

2012 年 Google 在 OSDI 上发表了 Spanner 的论文，2013 年在 SIGMOD 发表了 F1 的论文。这两篇论文让业界第一次看到了关系模型和 NoSQL 的扩展性在超庞大集群规模上融合的可能性。在此之前，大家普遍认为这个是不可能的，即使是 Google 也经历了 Megastore 这样的失败。

### Spanner综述
但是 Spanner 的创新之处在于通过硬件（GPS时钟+原子钟）来解决时钟同步的问题。在分布式系统里，时钟是最让人头痛的问题，刚才提到了 C* 为什么不是一个强 C 的系统，正是因为时钟的问题。而 Spanner 的厉害之处在于即使两个数据中心隔得非常远，不需要有通信（因为通信的代价太大，最快也就是光速）就能保证 TrueTime API的时钟误差在一个很小的范围内（10ms）。另外 Spanner 沿用了很多 Bigtable 的设计，比如 Tablet / Directory 等，同时在 Replica 这层使用 Paxos 复制，并未完全依赖底层的分布式文件系统。但是 Spanner 的设计底层仍然沿用了 Colossus，不过论文里也说是可以未来改进的点。

Google 的内部的数据库存储业务，大多是 3～5 副本，重要一点的 7 副本，遍布全球各大洲的数据中心，由于普遍使用了 Paxos，延迟是可以缩短到一个可以接受的范围（Google 的风格一向是追求吞吐的水平扩展而不是低延迟，从悲观锁的选择也能看得出来，因为跨数据中心复制是必选的，延迟不可能低，对于低延迟的场景，业务层自己解决或者依赖缓存）。另外由 Paxos 带来的 Auto-Failover 能力，更是能让整个集群即使数据中心瘫痪，业务层都是透明无感知的。另外 F1 构建在 Spanner 之上，对外提供了更丰富的 SQL 语法支持，F1 更像一个分布式 MPP SQL——F1 本身并不存储数据，而是将客户端的 SQL 翻译成类似 MapReduce 的任务，调用 Spanner 来完成请求。

其实 Spanner 和 F1 除了 TrueTime 整个系统并没有用什么全新的算法，其意义在于这是近些年来第一个 NewSQL 在生产环境中提供服务的分布式系统技术。

Spanner 和 F1 有以下几个重点：

完整的 SQL 支持，ACID 事务；
弹性伸缩能力；
自动的故障转移和故障恢复，多机房异地灾备。

NewSQL 特性确实非常诱人，在 Google 内部，大量的业务已经从原来的 Bigtable 切换到 Spanner 之上。我相信未来几年，整个业界的趋势也是如此，就像当年的 Hadoop 一样，Google 的基础软件的技术趋势是走在社区前面的。

### CockroachDB
CockroachDB 的技术选型比较激进，比如依赖了 HLC 来做事务的时间戳。但是在 Spanner 的事务模型的 Commit Wait 阶段等待时间的选择，CockroachDB 并没有办法做到 10ms 内的延迟；CockroachDB 的 Commit Wait 需要用户自己指定，但是谁能拍胸脯说 NTP 的时钟误差在多少毫秒内？我个人认为在处理跨洲际机房时钟同步的问题上，基本只有硬件时钟一种办法。HLC 是没办法解决的。另外 Cockroach 采用了 gossip 来同步节点信息，当集群变得比较大的时候，gossip 心跳会是一个非常大的开销。当然 CockroachDB 的这些技术选择带来的优势就是非常好的易用性，所有逻辑都在一个 binary 中，开箱即用，这个是非常大的优点。

### TiDB
目前从全球范围来看，另一个朝着 Spanner / F1 的开源实现这个目标上走的产品是 TiDB（终于谈到我们的产品了）。TiDB 本质上是一个更加正统的 Spanner 和 F1 实现，并不像 CockroachDB 那样选择将 SQL 和 Key-Value 融合，而是像 Spanner 和 F1 一样选择分离，这样分层的思想也是贯穿整个 TiDB 项目始终的。对于测试、滚动升级以及各层的复杂度控制会比较有优势；另外 TiDB 选择了 MySQL 协议和语法的兼容，MySQL 社区的 ORM 框架和运维工具，直接可以应用在 TiDB 上。

和 F1 一样，TiDB 是一个无状态的 MPP SQL Layer，整个系统的底层是依赖 TiKV 来提供分布式存储和分布式事务的支持。TiKV 的分布式事务模型采用的是 Google Percolator 的模型，但是在此之上做了很多优化。Percolator 的优点是去中心化程度非常高，整个集群不需要一个独立的事务管理模块，事务提交状态这些信息其实是均匀分散在系统的各个 Key 的 meta 中，整个模型唯一依赖的是一个授时服务器。在我们的系统上，极限情况这个授时服务器每秒能分配 400w 以上个单调递增的时间戳，大多数情况基本够用了（毕竟有 Google 量级的场景并不多见）；同时在 TiKV 中，这个授时服务本身是高可用的，也不存在单点故障的问题。

TiKV 和 CockroachDB 一样也是选择了 Raft 作为整个数据库的基础；不一样的是，TiKV 整体采用Rust 语言开发，作为一个没有 GC 和 Runtime 的语言，在性能上可以挖掘的潜力会更大。

### 关于未来
我觉得未来的数据库会有几个趋势，也是 TiDB 项目追求的目标：

数据库会随着业务云化，未来一切的业务都会跑在云端，不管是私有云、公有云还是混合云，运维团队接触的可能再也不是真实的物理机，而是一个个隔离的容器或者「计算资源」。这对数据库也是一个挑战，因为数据库天生就是有状态的，数据总是要存储在物理的磁盘上，而移动数据的代价比移动容器的代价可能大很多。

多租户技术会成为标配，一个大数据库承载一切的业务，数据在底层打通，上层通过权限，容器等技术进行隔离；但是数据的打通和扩展会变得异常简单，结合第一点提到的云化，业务层可以再也不用关心物理机的容量和拓扑，只需要认为底层是一个无穷大的数据库平台即可，不用再担心单机容量和负载均衡等问题。

OLAP 和 OLTP 会进一步细分，底层存储也许会共享一套，但是SQL优化器这层的实现一定是千差万别的。对于用户而言，如果能使用同一套标准的语法和规则来进行数据的读写和分析，会有更好的体验。

在未来分布式数据库系统上，主从日志同步这样落后的备份方式会被 Multi-Paxos / Raft 这样更强的分布式一致性算法替代，人工的数据库运维在管理大规模数据库集群时是不可能的，所有的故障恢复和高可用都会是高度自动化的。
